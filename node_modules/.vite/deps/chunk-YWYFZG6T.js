import {
  a,
  c,
  u as u2
} from "./chunk-TMILTRNC.js";
import {
  i
} from "./chunk-PNLWREZJ.js";
import {
  u
} from "./chunk-3IISYLTT.js";
import {
  e
} from "./chunk-C6PUUIY2.js";
import {
  e as e2
} from "./chunk-BQL7VXQR.js";
import {
  o as o2
} from "./chunk-735C75BI.js";
import {
  n as n2,
  o
} from "./chunk-ZLTZ3KAT.js";
import {
  O
} from "./chunk-RUM3DXFX.js";
import {
  r as r2
} from "./chunk-75RMBUYZ.js";
import {
  n
} from "./chunk-ZEEU5HOK.js";
import {
  l
} from "./chunk-NDQ5FHGV.js";
import {
  r
} from "./chunk-MIHB3CIK.js";

// node_modules/@arcgis/core/chunks/LineCallout.glsl.js
function S(o3) {
  const r3 = new o();
  r3.include(c), r3.include(u2, o3), r3.include(u, o3), r3.attributes.add(O.UV0, "vec2");
  const { vertex: S2, fragment: u4 } = r3;
  return S2.uniforms.add([new e("viewport", (e3, i2) => i2.camera.fullViewport), new o2("lineSize", (e3, i2) => Math.ceil(e3.size) * i2.camera.pixelRatio), new e2("pixelToNDC", (e3, o4) => r2(m, 2 / o4.camera.fullViewport[2], 2 / o4.camera.fullViewport[3])), new o2("borderSize", (i2, o4) => r(i2.borderColor) ? o4.camera.pixelRatio : 0), new e2("screenOffset", (e3, o4) => r2(m, e3.screenOffset[0] * o4.camera.pixelRatio, e3.screenOffset[1] * o4.camera.pixelRatio))]), r3.varyings.add("coverageSampling", "vec4"), r3.varyings.add("lineSizes", "vec2"), o3.hasMultipassGeometry && r3.varyings.add("depth", "float"), o3.hasScreenSizePerspective && i(S2), S2.code.add(n2`
    void main(void) {
      ProjectHUDAux projectAux;
      vec4 endPoint = projectPositionHUD(projectAux);

      vec3 vpos = projectAux.posModel;
      if (rejectBySlice(vpos)) {
        gl_Position = vec4(1e38, 1e38, 1e38, 1.0);
        return;
      }
    ${o3.occlusionTestEnabled ? n2`
      if (!testVisibilityHUD(endPoint)) {
        gl_Position = vec4(1e38, 1e38, 1e38, 1.0);
        return;
      }` : ""}

    ${o3.hasScreenSizePerspective ? n2`
      vec4 perspectiveFactor = screenSizePerspectiveScaleFactor(projectAux.absCosAngle, projectAux.distanceToCamera, screenSizePerspectiveAlignment);
      vec2 screenOffsetScaled = applyScreenSizePerspectiveScaleFactorVec2(screenOffset, perspectiveFactor);
        ` : n2`
      vec2 screenOffsetScaled = screenOffset;
        `}
      // Add view dependent polygon offset to get exact same original starting point. This is mostly
      // used to get the correct depth value
      vec3 posView = (view * vec4(position, 1.0)).xyz;
      ${o3.hasMultipassGeometry ? "depth = posView.z;" : ""}

      applyHUDViewDependentPolygonOffset(auxpos1.w, projectAux.absCosAngle, posView);
      vec4 startPoint = proj * vec4(posView, 1.0);
      // Apply screen offset to both start and end point
      vec2 screenOffsetNorm = screenOffsetScaled * 2.0 / viewport.zw;
      startPoint.xy += screenOffsetNorm * startPoint.w;
      endPoint.xy += screenOffsetNorm * endPoint.w;
      // Align start and end to pixel origin
      vec4 startAligned = alignToPixelOrigin(startPoint, viewport.zw);
      vec4 endAligned = alignToPixelOrigin(endPoint, viewport.zw);
    ${o3.depthHudEnabled ? o3.depthHudAlignStartEnabled ? n2`endAligned = vec4(endAligned.xy / endAligned.w * startAligned.w, startAligned.zw);` : n2`startAligned = vec4(startAligned.xy / startAligned.w * endAligned.w, endAligned.zw);` : ""}
      vec4 projectedPosition = mix(startAligned, endAligned, uv0.y);
      // The direction of the line in screen space
      vec2 screenSpaceDirection = normalize(endAligned.xy / endAligned.w - startAligned.xy / startAligned.w);
      vec2 perpendicularScreenSpaceDirection = vec2(screenSpaceDirection.y, -screenSpaceDirection.x);
    ${o3.hasScreenSizePerspective ? n2`
      float lineSizeScaled = applyScreenSizePerspectiveScaleFactorFloat(lineSize, perspectiveFactor);
      float borderSizeScaled = applyScreenSizePerspectiveScaleFactorFloat(borderSize, perspectiveFactor);
        ` : n2`
      float lineSizeScaled = lineSize;
      float borderSizeScaled = borderSize;
        `}
      float halfPixelSize = lineSizeScaled * 0.5;
      // Calculate a pixel offset from the edge of the pixel, s.t. we keep the line aligned
      // to pixels if it has a full pixel size. Since pixel aligned biases to the bottom-left,
      // we bias the size to the right (for odd sizes) to balance out the bias. Grow sub-pixel
      // sizes towards the left or right s.t. there is a smooth transition (e.g. from 2 to 3 px).
      float halfWholePixelSize = floor(lineSizeScaled) * 0.5;
      float halfPixelSizeInt = floor(halfWholePixelSize);

      // Sub-pixel offset if we need to grow sub-pixels to the left
      float subpixelOffset = -fract(lineSizeScaled) * float(halfWholePixelSize > 0.0);

      // Pixel offset aligning to whole pixels and adding subpixel offset if needed
      float pixelOffset = -halfPixelSizeInt + subpixelOffset;

      // Compute full ndc offset, adding 1px padding for doing anti-aliasing and the border size
      float padding = 1.0 + borderSizeScaled;
      vec2 ndcOffset = (pixelOffset - padding + uv0.x * (lineSizeScaled + padding + padding)) * pixelToNDC;

      // Offset x/y from the center of the line in screen space
      projectedPosition.xy += perpendicularScreenSpaceDirection * ndcOffset * projectedPosition.w;

      // Compute a coverage varying which we can use in the fragment shader to determine
      // how much a pixel is actually covered by the line (i.e. to anti alias the line).
      // This works by computing two coordinates that can be linearly interpolated and then
      // subtracted to find out how far away from the line edge we are.
      float edgeDirection = (uv0.x * 2.0 - 1.0);

      float halfBorderSize = 0.5 * borderSizeScaled;
      float halfPixelSizeAndBorder = halfPixelSize + halfBorderSize;
      float outerEdgeCoverageSampler = edgeDirection * (halfPixelSizeAndBorder + halfBorderSize + 1.0);

      float isOneSided = float(lineSizeScaled < 2.0 && borderSize < 2.0);

      coverageSampling = vec4(
        // Edge coordinate
        outerEdgeCoverageSampler,

        // Border edge coordinate
        outerEdgeCoverageSampler - halfPixelSizeAndBorder * isOneSided,

        // Line offset
        halfPixelSize - 0.5,

        // Border offset
        halfBorderSize - 0.5 + halfPixelSizeAndBorder * (1.0 - isOneSided)
      );

      lineSizes = vec2(lineSizeScaled, borderSizeScaled);

      gl_Position = projectedPosition;
    }
  `), u4.uniforms.add([new e("uColor", (e3) => v(e3.color)), new e("borderColor", (e3) => v(e3.borderColor))]), o3.hasMultipassGeometry && (u4.include(a, o3), u4.uniforms.add(new e2("inverseViewport", (e3, i2) => i2.inverseViewport))), u4.code.add(n2`
    void main() {
      ${o3.hasMultipassGeometry ? "if( geometryDepthTest(gl_FragCoord.xy * inverseViewport, depth) ){ discard; }" : ""}

      // Mix between line and border coverage offsets depending on whether we need
      // a border (based on the sidedness).
      vec2 coverage = min(1.0 - clamp(abs(coverageSampling.xy) - coverageSampling.zw, 0.0, 1.0), lineSizes);

      // Mix between border and line color based on the line coverage (conceptually the line
      // blends on top of the border background).
      //
      // Anti-alias by blending final result using the full (including optional border) coverage
      // and the color alpha
      float borderAlpha = uColor.a * borderColor.a * coverage.y;
      float colorAlpha = uColor.a * coverage.x;

      float finalAlpha = mix(borderAlpha, 1.0, colorAlpha);

    ${o3.depthHudEnabled ? n2`
      if (finalAlpha < 0.01) {
        discard;
      }
      ` : n2`
      vec3 finalRgb = mix(borderColor.rgb * borderAlpha, uColor.rgb, colorAlpha);
      gl_FragColor = vec4(finalRgb, finalAlpha);
      `}
  }
  `), r3;
}
function v(i2) {
  return r(i2) ? i2 : l;
}
var m = n();
var u3 = Object.freeze(Object.defineProperty({ __proto__: null, build: S }, Symbol.toStringTag, { value: "Module" }));

export {
  S,
  u3 as u
};
//# sourceMappingURL=chunk-YWYFZG6T.js.map
